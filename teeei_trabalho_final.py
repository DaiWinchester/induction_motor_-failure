# -*- coding: utf-8 -*-
"""TEEEI-trabalho_final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Hwk3kAKmzow3CTm43GOjJIG5IG25eG_3

**Equipe:**

Dailneide Costa Ribeiro - 202106840014

Saulo

Raimundo

### Informações dos dados

Dados:

https://www.lias-lab.fr/mcsa/

Banco de ensaio 1: Motor de indução - Falha na barra do rotor

Variáveis (em ordem):
- time_secunds
- corrente_estator_isa (Ampere)
- corrente_estator_isb (Ampere)
- corrente_estator_isc (Ampere)
- tensao_estator_Vsa (Volts)
- tensao_estator_Vsb (Volts)
- tensao_estator_Vsc (Volts)
- velocidade (rad/s)
- posicao_rotor (rad)

Os arquivos de dados contêm as seguintes colunas, nesta ordem exata:

    Time (Tempo):

        Unidade: Segundos (s).

        Descrição: O registro temporal da amostragem dos dados durante o experimento.

    Stator Currents (Correntes do Estator - isa, isb, isc):

        Unidade: Ampere (A).

        Descrição: São as correntes elétricas medidas nas três fases do estator do motor. Geralmente utilizadas para analisar a assinatura da corrente (MCSA) e identificar anomalias nas frequências associadas a falhas.

        Variáveis: isa, isb, isc.

    Stator Voltages (Tensões do Estator - Vsa, Vsb, Vsc):

        Unidade: Volt (V).

        Descrição: As tensões de alimentação aplicadas às três fases do estator. Úteis para verificar o equilíbrio da alimentação e para cálculos de potência ou impedância.

        Variáveis: Vsa, Vsb, Vsc.

    Speed (Velocidade):

        Unidade: Radianos por segundo (rad/s).

        Descrição: A velocidade angular de rotação do eixo do motor no momento da medição.

    Rotor mechanical position (Posição mecânica do rotor):

        Unidade: Radianos (rad).

        Descrição: A posição angular absoluta ou relativa do rotor, fundamental para análises que dependem do ângulo de fase ou controle vetorial.

### Importações
"""

import pandas as pd
import os
import glob

"""### Carregar dados"""

caminho_dentro_do_repo = "organized_data/motor_data_organized"
caminho_completo = os.path.join('.', caminho_dentro_do_repo)

COLUNAS_MCSA = [
    'time_secunds',
    'Is_a', 'Is_b', 'Is_c',
    'Vs_a', 'Vs_b', 'Vs_c',
    'Speed', 'Position'
]

def criar_dataset_github(caminho_pasta):
    # Procura por todos os arquivos .csv na pasta clonada
    padrao_busca = os.path.join(caminho_pasta, "*.csv")
    arquivos = glob.glob(padrao_busca)

    if not arquivos:
        print(f"Erro: Nenhum CSV encontrado em: {caminho_pasta}")
        print("Verifique se o caminho da pasta está correto dentro do repositório.")
        return None
    print(f"Encontrados {len(arquivos)} arquivos. Processando...")

    lista_dfs = []
    for arquivo in arquivos:
        # Lê o arquivo localmente
        df_temp = pd.read_csv(arquivo, header=None, names=COLUNAS_MCSA)
        lista_dfs.append(df_temp)
    # Junta tudo num só
    dataset_final = pd.concat(lista_dfs, axis=0, ignore_index=True)
    return dataset_final

"""### Preparação dos datasets

Ao final temos os seguintes datasets:
- df_1bar_medium, df_1bar_high, df_1bar_low
- df_2bar_medium, df_2bar_high, df_2bar_low
- df_healthy_medium, df_healthy_high, df_healthy_low

- df_healthy_completo, df_1bar_completo, df_2bar_completo

- df_completo

#### Medium_Speed
"""

# Pastas do drive
pasta_1 = 'Healthy/Medium_Speed'
pasta_2 = '1_Broken_Bar/Medium_Speed'
pasta_3 = '2_Broken_Bars/Medium_Speed'

path_healthy = os.path.join(caminho_completo, pasta_1)
path_1bar = os.path.join(caminho_completo, pasta_2)
path_2bar = os.path.join(caminho_completo, pasta_3)

df_healthy_medium = criar_dataset_github(path_healthy)
df_1bar_medium = criar_dataset_github(path_1bar)
df_2bar_medium = criar_dataset_github(path_2bar)

"""#### High_Speed"""

# Pastas do drive
pasta_1 = 'Healthy/High_Speed'
pasta_2 = '1_Broken_Bar/High_Speed'
pasta_3 = '2_Broken_Bars/High_Speed'

path_healthy = os.path.join(caminho_completo, pasta_1)
path_1bar = os.path.join(caminho_completo, pasta_2)
path_2bar = os.path.join(caminho_completo, pasta_3)

df_healthy_high = criar_dataset_github(path_healthy)
df_1bar_high = criar_dataset_github(path_1bar)
df_2bar_high = criar_dataset_github(path_2bar)


"""#### Low_Speed"""

# Pastas do drive
pasta_1 = 'Healthy/Low_Speed'
pasta_2 = '1_Broken_Bar/Low_Speed'
pasta_3 = '2_Broken_Bars/Low_Speed'

path_healthy = os.path.join(caminho_completo, pasta_1)
path_1bar = os.path.join(caminho_completo, pasta_2)
path_2bar = os.path.join(caminho_completo, pasta_3)

df_healthy_low = criar_dataset_github(path_healthy)
df_1bar_low = criar_dataset_github(path_1bar)
df_2bar_low = criar_dataset_github(path_2bar)


"""#### Dataset completo (todos os dados)"""

df_healthy_completo = pd.concat([df_healthy_medium, df_healthy_high, df_healthy_low], axis=0, ignore_index=True)
df_1bar_completo = pd.concat([df_1bar_medium, df_1bar_high, df_1bar_low], axis=0, ignore_index=True)
df_2bar_completo = pd.concat([df_2bar_medium, df_2bar_high, df_2bar_low], axis=0, ignore_index=True)

df_healthy_completo["class"] = "healthy"
df_1bar_completo["class"] = "1bar"
df_2bar_completo["class"] = "2bar"

df_healthy_completo.head()

df_completo = pd.concat([df_1bar_completo, df_healthy_completo, df_2bar_completo], axis=0, ignore_index=True)
df_completo.head()

"""### Tratamento dos dados"""

# Verificar valores ausentes
# df_completo.isna().sum() -> não tem valores nulos

df_completo.groupby("class").size()

df_completo.info(
    verbose=True,
    show_counts=True
)

df_temp = df_completo.copy()
df_temp = df_temp.drop(columns=["class"])
# df_temp.corr()

# Normalização dos dados (verificar se é necessário)

"""### Aplicação da FFT"""

# Aplicar FFT (usar df_completo)

"""### Divisão dos dados e Treinamento do modelo"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

y = df_completo["class"]
X = df_completo.drop(columns=["class"])

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, random_state = 42)

from sklearn.ensemble import RandomForestClassifier

n_estimators = [10, 50, 80, 100, 150]

for i in n_estimators:
  model = RandomForestClassifier(n_estimators=i, random_state=42)

  model.fit(X_train,y_train)
  y_pred = model.predict(X_test)
  # Métricas: precisão, recall e f1-score
  print(f'Resultados para n_estimators = {i}')
  print(classification_report(y_test, y_pred, digits=3))

"""### Resultados"""

# Matriz de confusão
labels = np.unique(y)
cm = confusion_matrix(y_test, y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=labels)
disp.plot()
plt.title('Matriz de Confusão')
plt.show()

# gráfico de acurácia

"""###Conclusão"""